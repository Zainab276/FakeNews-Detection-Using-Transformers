{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0012b01a",
   "metadata": {},
   "source": [
    "# Fake News Detection — Ensemble of Transformers (BERT + RoBERTa)\n",
    "\n",
    "This notebook builds a **fake news detector** using advanced AI techniques: fine-tuning pretrained **Transformer** models (DistilBERT / BERT and RoBERTa) and combining them in an ensemble. The dataset used is the **'Fake and Real News'** dataset from Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2427339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.57.0\n",
      "datasets 4.1.1\n",
      "sklearn 1.7.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify versions\n",
    "import transformers, datasets, sklearn\n",
    "print('transformers', transformers.__version__)\n",
    "print('datasets', datasets.__version__)\n",
    "print('sklearn', sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7a0c7",
   "metadata": {},
   "source": [
    "## 1) Download dataset from Kaggle\n",
    "\n",
    "The Kaggle dataset used: **Fake and Real News Dataset** — two CSV files (`Fake.csv` and `True.csv`).\n",
    "\n",
    "Kaggle dataset page: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98e91397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset loaded successfully!\n",
      "Total records: 44898\n",
      "                                               title  \\\n",
      "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
      "1  Trump drops Steve Bannon from National Securit...   \n",
      "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
      "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
      "4  Donald Trump heads for Scotland to reopen a go...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
      "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
      "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
      "3  On Monday, Donald Trump once again embarrassed...          News   \n",
      "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
      "\n",
      "                  date  label  \n",
      "0    February 13, 2017      0  \n",
      "1       April 5, 2017       1  \n",
      "2  September 27, 2017       1  \n",
      "3         May 22, 2017      0  \n",
      "4       June 24, 2016       1  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset directly from local files\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure data folder exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Load both datasets\n",
    "fake_df = pd.read_csv(\"Fake.csv\")\n",
    "true_df = pd.read_csv(\"True.csv\")\n",
    "\n",
    "# Add labels\n",
    "fake_df[\"label\"] = 0\n",
    "true_df[\"label\"] = 1\n",
    "\n",
    "# Combine both into one dataframe\n",
    "df = pd.concat([fake_df, true_df], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\" Dataset loaded successfully!\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745675bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 44898\n",
      "Label distribution:\n",
      " label\n",
      "fake    23481\n",
      "real    21417\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date label  \n",
       "0  December 31, 2017  fake  \n",
       "1  December 31, 2017  fake  \n",
       "2  December 30, 2017  fake  \n",
       "3  December 29, 2017  fake  \n",
       "4  December 25, 2017  fake  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "fake_path = 'Fake.csv'\n",
    "true_path = 'True.csv'\n",
    "if os.path.exists(fake_path) and os.path.exists(true_path):\n",
    "    fake = pd.read_csv(fake_path)\n",
    "    true = pd.read_csv(true_path)\n",
    "    df = pd.concat([fake.assign(label='fake'), true.assign(label='real')], ignore_index=True)\n",
    "    \n",
    "    print('Total samples:', len(df))\n",
    "    print('Label distribution:\\n', df['label'].value_counts())\n",
    "    display(df.head())\n",
    "else:\n",
    "    print('Data files not found. Place Fake.csv and True.csv into the ./data folder.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a350bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As World Trade Center Fell, Donald Trump Boas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC mayor warns Trump: 'stop and frisk' will m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. options market not very 'Trumped up' ahea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Electric vehicle sales fall far short of Obama...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OBAMACARE: Your Dog Might Have Better Healthca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0   As World Trade Center Fell, Donald Trump Boas...      0\n",
       "1  NYC mayor warns Trump: 'stop and frisk' will m...      0\n",
       "2  U.S. options market not very 'Trumped up' ahea...      0\n",
       "3  Electric vehicle sales fall far short of Obama...      0\n",
       "4  OBAMACARE: Your Dog Might Have Better Healthca...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if 'df' in globals():\n",
    "    def combine_text(row):\n",
    "        parts = []\n",
    "        if 'title' in row and pd.notna(row['title']):\n",
    "            parts.append(str(row['title']))\n",
    "        if 'text' in row and pd.notna(row['text']):\n",
    "            parts.append(str(row['text']))\n",
    "        return '\\n'.join(parts)\n",
    "\n",
    "    df['text_all'] = df.apply(combine_text, axis=1)\n",
    "    df = df[['text_all','label']].rename(columns={'text_all':'text'})\n",
    "    df['label_id'] = (df['label']=='real').astype(int)\n",
    "    dataset = Dataset.from_pandas(df[['text','label_id']].rename(columns={'label_id':'label'}))\n",
    "    display(dataset.shuffle(seed=42).select(range(5)).to_pandas())\n",
    "else:\n",
    "    print('Dataset not prepared because original CSVs are missing.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12bace6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8a5b5d593a41ec95a46bc963acb64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450691f89cc2441e9a244638f56d22f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595e8dbc26a646bdbe4165f3ca912965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c52a0d0014070822dca9c34f4111a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fbbbf1214f49cf836c8781bf9e73d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Model names\n",
    "model_name_1 = 'distilbert-base-uncased'\n",
    "model_name_2 = 'roberta-base'\n",
    "\n",
    "# Tokenizers\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name_1)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name_2)\n",
    "\n",
    "# Define tokenization functions\n",
    "def tokenize1(batch):\n",
    "    return tokenizer1(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "def tokenize2(batch):\n",
    "    return tokenizer2(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# --- Widget setup (compatible with JupyterLab 4.4.9) ---\n",
    "info_box = widgets.Output()\n",
    "with info_box:\n",
    "    display(HTML(\"<b>Initializing tokenization process...</b>\"))\n",
    "\n",
    "display(info_box)\n",
    "\n",
    "# --- Main tokenization workflow ---\n",
    "try:\n",
    "    if 'dataset' in globals():\n",
    "        with info_box:\n",
    "            print(\"Dataset found. Splitting and tokenizing...\")\n",
    "        ds = dataset.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "        # Smaller demo sample (optional)\n",
    "        DEMO = True\n",
    "        if DEMO:\n",
    "            ds['train'] = ds['train'].shuffle(seed=42).select(range(min(2000, len(ds['train']))))\n",
    "            ds['test'] = ds['test'].shuffle(seed=42).select(range(min(500, len(ds['test']))))\n",
    "\n",
    "        tokenized1 = ds.map(tokenize1, batched=True)\n",
    "        tokenized2 = ds.map(tokenize2, batched=True)\n",
    "\n",
    "        with info_box:\n",
    "            print(\" Tokenization completed successfully.\")\n",
    "    else:\n",
    "        with info_box:\n",
    "            print(\"Tokenization skipped — dataset not available.\")\n",
    "except Exception as e:\n",
    "    with info_box:\n",
    "        print(f\" Error during tokenization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5bf16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainers initialized. To start training run `trainer1.train()` and `trainer2.train()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN10\\AppData\\Local\\Temp\\ipykernel_1504\\2121677165.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer1 = Trainer(\n",
      "C:\\Users\\WIN10\\AppData\\Local\\Temp\\ipykernel_1504\\2121677165.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer2 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "if 'tokenized1' in globals() and 'tokenized2' in globals():\n",
    "    model1 = AutoModelForSequenceClassification.from_pretrained(model_name_1, num_labels=2)\n",
    "    model2 = AutoModelForSequenceClassification.from_pretrained(model_name_2, num_labels=2)\n",
    "\n",
    "    # Faster training parameters\n",
    "    args1 = TrainingArguments(\n",
    "        output_dir='models/distilbert',\n",
    "        per_device_train_batch_size=8,   # reduced batch size\n",
    "        per_device_eval_batch_size=16,   # reduced batch size\n",
    "        num_train_epochs=1,              # reduced epochs\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "    )\n",
    "    setattr(args1, \"do_eval\", True)\n",
    "    setattr(args1, \"evaluation_strategy\", \"epoch\")\n",
    "    setattr(args1, \"save_strategy\", \"epoch\")\n",
    "    setattr(args1, \"save_total_limit\", 2)\n",
    "\n",
    "    args2 = TrainingArguments(\n",
    "        output_dir='models/roberta',\n",
    "        per_device_train_batch_size=4,   # reduced batch size\n",
    "        per_device_eval_batch_size=16,   # reduced batch size\n",
    "        num_train_epochs=1,              # reduced epochs\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "    )\n",
    "    setattr(args2, \"do_eval\", True)\n",
    "    setattr(args2, \"evaluation_strategy\", \"epoch\")\n",
    "    setattr(args2, \"save_strategy\", \"epoch\")\n",
    "    setattr(args2, \"save_total_limit\", 2)\n",
    "\n",
    "    trainer1 = Trainer(\n",
    "        model=model1,\n",
    "        args=args1,\n",
    "        train_dataset=tokenized1['train'],\n",
    "        eval_dataset=tokenized1['test'],\n",
    "        tokenizer=tokenizer1,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer2 = Trainer(\n",
    "        model=model2,\n",
    "        args=args2,\n",
    "        train_dataset=tokenized2['train'],\n",
    "        eval_dataset=tokenized2['test'],\n",
    "        tokenizer=tokenizer2,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\" Trainers initialized. To start training run `trainer1.train()` and `trainer2.train()`.\")\n",
    "else:\n",
    "    print('Training setup skipped because tokenized datasets are not present.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58602c6b-2c34-45fd-ba8e-49da80e657a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN10\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 54:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN10\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 2:06:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.009807163601275534, metrics={'train_runtime': 7607.8104, 'train_samples_per_second': 0.263, 'train_steps_per_second': 0.066, 'total_flos': 263111055360000.0, 'train_loss': 0.009807163601275534, 'epoch': 1.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer1.train()\n",
    "trainer2.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3aaed",
   "metadata": {},
   "source": [
    "## Ensemble inference (average logits)\n",
    "\n",
    "After fine-tuning both models, you can produce predictions by averaging the output logits (or probabilities) from both models and taking the argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8d59301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble inference function defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def ensemble_predict(texts, model1, tokenizer1, model2, tokenizer2, device=None):\n",
    "    enc1 = tokenizer1(texts, truncation=True, padding=True, return_tensors='pt', max_length=256)\n",
    "    enc2 = tokenizer2(texts, truncation=True, padding=True, return_tensors='pt', max_length=256)\n",
    "    if device is not None:\n",
    "        model1.to(device)\n",
    "        model2.to(device)\n",
    "        enc1 = {k:v.to(device) for k,v in enc1.items()}\n",
    "        enc2 = {k:v.to(device) for k,v in enc2.items()}\n",
    "    with torch.no_grad():\n",
    "        out1 = model1(**enc1).logits.cpu().numpy()\n",
    "        out2 = model2(**enc2).logits.cpu().numpy()\n",
    "    avg = (out1 + out2) / 2.0\n",
    "    preds = avg.argmax(axis=1)\n",
    "    return preds\n",
    "\n",
    "print('Ensemble inference function defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "916ba7e0-cede-48bb-a7a8-8607a8c3874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted numeric labels: [0 0]\n",
      "Predicted text labels: ['Fake News', 'Fake News']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensemble prediction function\n",
    "def ensemble_predict(texts, model1, tokenizer1, model2, tokenizer2, device=None):\n",
    "    enc1 = tokenizer1(texts, truncation=True, padding=True, return_tensors='pt', max_length=256)\n",
    "    enc2 = tokenizer2(texts, truncation=True, padding=True, return_tensors='pt', max_length=256)\n",
    "    \n",
    "    if device is not None:\n",
    "        model1.to(device)\n",
    "        model2.to(device)\n",
    "        enc1 = {k:v.to(device) for k,v in enc1.items()}\n",
    "        enc2 = {k:v.to(device) for k,v in enc2.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out1 = model1(**enc1).logits.cpu().numpy()\n",
    "        out2 = model2(**enc2).logits.cpu().numpy()\n",
    "    \n",
    "    avg = (out1 + out2) / 2.0\n",
    "    preds = avg.argmax(axis=1)\n",
    "    return preds\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"The government announced a new healthcare policy today.\",\n",
    "    \"Aliens landed in London and started playing football!\"\n",
    "]\n",
    "\n",
    "# Run ensemble prediction\n",
    "preds = ensemble_predict(texts, model1, tokenizer1, model2, tokenizer2, device=device)\n",
    "\n",
    "# Map numeric predictions to labels (optional)\n",
    "label_names = {0: \"Fake News\", 1: \"Real News\"}\n",
    "pred_labels = [label_names[p] for p in preds]\n",
    "\n",
    "print(\"Predicted numeric labels:\", preds)\n",
    "print(\"Predicted text labels:\", pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903dac2f-2020-41f7-84a0-a93aa8a69f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
